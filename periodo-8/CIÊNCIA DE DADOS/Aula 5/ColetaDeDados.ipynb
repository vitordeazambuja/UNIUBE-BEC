{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKOFK_ahqHIR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aula - Coleta de Dados para Ciência de Dados\n",
        "\n",
        "prof: clenio.silva@uniube.br"
      ],
      "metadata": {
        "id": "3ehP4EW8qL-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando bibliotecas necessárias\n",
        "import requests # para requisições HTTP\n",
        "import pandas as pd # para manipulação de dados em formato tabular\n",
        "from bs4 import BeautifulSoup # para web scraping\n",
        "import json # para trabalhar com JSON\n",
        "import os # para operações com sistema de arquivos"
      ],
      "metadata": {
        "id": "GMpr_7sgqN0e"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Introdução sobre a Coleta de Dados\n",
        "\n",
        "\"\"\"\n",
        "A coleta de dados é a primeira etapa do processo de Ciência de Dados. Nesta etapa, os cientistas de dados buscam\n",
        "dados de várias fontes, que podem incluir bancos de dados, APIs, arquivos locais, scraping de websites, entre outros.\n",
        "Esta etapa é crucial, pois a qualidade e a relevância dos dados coletados impactam diretamente as análises e modelagens subsequentes.\n",
        "\"\"\"\n",
        "\n",
        "# Vamos agora explorar algumas maneiras comuns de coletar dados em Python:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "iorRIi2bqfZ8",
        "outputId": "952b37c7-3452-443d-b476-7edb030bb323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nA coleta de dados é a primeira etapa do processo de Ciência de Dados. Nesta etapa, os cientistas de dados buscam\\ndados de várias fontes, que podem incluir bancos de dados, APIs, arquivos locais, scraping de websites, entre outros.\\nEsta etapa é crucial, pois a qualidade e a relevância dos dados coletados impactam diretamente as análises e modelagens subsequentes.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Coletando dados via API\n",
        "\n",
        "# Exemplo usando uma API pública (por exemplo, JSONPlaceholder)\n",
        "url = \"https://jsonplaceholder.typicode.com/posts\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificando o status da requisição\n",
        "if response.status_code == 200:\n",
        "    data = response.json()  # Convertendo o JSON retornado para um dicionário Python\n",
        "    print(\"Dados coletados com sucesso!\")\n",
        "else:\n",
        "    print(\"Falha na coleta de dados.\")\n",
        "\n",
        "# Exibindo as 5 primeiras entradas\n",
        "print(\"Primeiras entradas dos dados coletados via API:\")\n",
        "for post in data[:5]:\n",
        "    print(post)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6inbnzQqp51",
        "outputId": "79d82632-f837-4e04-a274-5feae25b4541"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados coletados com sucesso!\n",
            "Primeiras entradas dos dados coletados via API:\n",
            "{'userId': 1, 'id': 1, 'title': 'sunt aut facere repellat provident occaecati excepturi optio reprehenderit', 'body': 'quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto'}\n",
            "{'userId': 1, 'id': 2, 'title': 'qui est esse', 'body': 'est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi nulla'}\n",
            "{'userId': 1, 'id': 3, 'title': 'ea molestias quasi exercitationem repellat qui ipsa sit aut', 'body': 'et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel accusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut'}\n",
            "{'userId': 1, 'id': 4, 'title': 'eum et est occaecati', 'body': 'ullam et saepe reiciendis voluptatem adipisci\\nsit amet autem assumenda provident rerum culpa\\nquis hic commodi nesciunt rem tenetur doloremque ipsam iure\\nquis sunt voluptatem rerum illo velit'}\n",
            "{'userId': 1, 'id': 5, 'title': 'nesciunt quas odio', 'body': 'repudiandae veniam quaerat sunt sed\\nalias aut fugiat sit autem sed est\\nvoluptatem omnis possimus esse voluptatibus quis\\nest aut tenetur dolor neque'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Coletando dados de um arquivo CSV\n",
        "\n",
        "# Para este exemplo, vamos supor que temos um arquivo CSV com dados sobre vendas\n",
        "# Vamos usar o pandas para ler o arquivo CSV\n",
        "\n",
        "# Caminho para o arquivo CSV (substitua pelo caminho correto no seu ambiente)\n",
        "# Exemplo fictício de arquivo CSV:\n",
        "# caminho = \"dados_vendas.csv\"\n",
        "\n",
        "# Usando pandas para carregar o CSV\n",
        "# df = pd.read_csv(caminho)\n",
        "\n",
        "# Para fins de exemplo, criaremos um DataFrame simples\n",
        "df = pd.DataFrame({\n",
        "    'Produto': ['Produto A', 'Produto B', 'Produto C'],\n",
        "    'Vendas': [1200, 800, 1500],\n",
        "    'Data': ['2025-01-01', '2025-01-02', '2025-01-03']\n",
        "})\n",
        "\n",
        "# Exibindo o DataFrame\n",
        "print(\"\\nDados do arquivo CSV:\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt63Bucsqxlp",
        "outputId": "eaf34e66-c1b0-4cee-e0ba-07af3e36479d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dados do arquivo CSV:\n",
            "     Produto  Vendas        Data\n",
            "0  Produto A    1200  2025-01-01\n",
            "1  Produto B     800  2025-01-02\n",
            "2  Produto C    1500  2025-01-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Coletando dados via Web Scraping\n",
        "\n",
        "# Vamos agora ver como fazer o scraping de uma página web para coletar dados\n",
        "\n",
        "# Exemplo de scraping do site 'https://quotes.toscrape.com'\n",
        "url = 'https://quotes.toscrape.com'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificando se a requisição foi bem-sucedida\n",
        "if response.status_code == 200:\n",
        "    print(\"\\nRequisição bem-sucedida! Coletando dados da página...\")\n",
        "\n",
        "    # Usando BeautifulSoup para parsear o conteúdo HTML da página\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Encontrando todas as citações na página\n",
        "    quotes = soup.find_all('span', class_='text')\n",
        "\n",
        "    # Exibindo as 5 primeiras citações\n",
        "    print(\"\\nPrimeiras citações coletadas:\")\n",
        "    for quote in quotes[:5]:\n",
        "        print(quote.get_text())\n",
        "else:\n",
        "    print(\"Falha ao acessar a página para scraping.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMtC5EB2qznf",
        "outputId": "e580c485-f14f-48c4-9422-d3322bcb672c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Requisição bem-sucedida! Coletando dados da página...\n",
            "\n",
            "Primeiras citações coletadas:\n",
            "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
            "“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
            "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
            "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
            "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Coletando dados a partir de um arquivo JSON\n",
        "\n",
        "# Suponha que temos um arquivo JSON com informações sobre alunos\n",
        "# Exemplo de estrutura do JSON:\n",
        "# {\n",
        "#    \"alunos\": [\n",
        "#       {\"nome\": \"João\", \"nota\": 9.5},\n",
        "#       {\"nome\": \"Maria\", \"nota\": 8.7}\n",
        "#    ]\n",
        "# }\n",
        "\n",
        "# Vamos simular um arquivo JSON com os dados de alunos\n",
        "json_data = '''\n",
        "{\n",
        "    \"alunos\": [\n",
        "        {\"nome\": \"João\", \"nota\": 9.5},\n",
        "        {\"nome\": \"Maria\", \"nota\": 8.7},\n",
        "        {\"nome\": \"Carlos\", \"nota\": 7.3}\n",
        "    ]\n",
        "}\n",
        "'''\n",
        "\n",
        "# Carregando os dados JSON em um dicionário Python\n",
        "data_json = json.loads(json_data)\n",
        "\n",
        "# Exibindo os dados dos alunos\n",
        "print(\"\\nDados dos alunos coletados do JSON:\")\n",
        "for aluno in data_json['alunos']:\n",
        "    print(aluno)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bCXIwWKq6yX",
        "outputId": "771edf13-efe0-4d77-e951-decd3b5b47ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dados dos alunos coletados do JSON:\n",
            "{'nome': 'João', 'nota': 9.5}\n",
            "{'nome': 'Maria', 'nota': 8.7}\n",
            "{'nome': 'Carlos', 'nota': 7.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Boas práticas para coleta de dados\n",
        "\n",
        "\"\"\"\n",
        "- Certifique-se de que os dados coletados são relevantes e estão atualizados.\n",
        "- Verifique a licença e os termos de uso dos dados, especialmente quando estiver coletando dados de APIs ou sites.\n",
        "- Quando for fazer scraping, tome cuidado para não sobrecarregar o servidor da web e sempre siga as boas práticas de scraping.\n",
        "- Armazene os dados coletados de maneira eficiente, em formatos como CSV, JSON ou banco de dados.\n",
        "- Realize validação e limpeza dos dados após a coleta, pois é comum encontrar inconsistências ou dados faltantes.\n",
        "\"\"\"\n",
        "\n",
        "# 7. Conclusão\n",
        "\n",
        "\"\"\"\n",
        "Na coleta de dados, usamos diferentes fontes como APIs, arquivos CSV, JSON e scraping de páginas web. Após coletar os dados,\n",
        "é importante organizá-los e validá-los para garantir que estão prontos para análise e modelagem.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xxU3ESuZrFyH",
        "outputId": "a69d6ff3-1d27-42c6-d518-448a9b1fe7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNa coleta de dados, usamos diferentes fontes como APIs, arquivos CSV, JSON e scraping de páginas web. Após coletar os dados,\\né importante organizá-los e validá-los para garantir que estão prontos para análise e modelagem.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}